线性回归

学习目标：
解释损失函数及其运作方式。
定义并描述梯度下降如何找到最佳模型参数。
介绍如何调整超参数以高效训练线性模型。



选择最佳损失函数时，请考虑您希望模型如何处理离群值。



梯度下降法是一种数学技巧，可迭代地找到能使模型产生最低损失的权重和偏差。梯度下降法通过重复以下过程（迭代次数由用户定义）来找到最佳权重和偏差。

模型开始训练时，权重和偏差会随机化为接近于零的值，然后重复执行以下步骤：

使用当前权重和偏差计算损失。

确定可减少损失的权重和偏差的移动方向。

将权重和偏差值沿可减少损失的方向移动少量距离。

返回到第 1 步，重复该过程，直到模型无法进一步减少损失为止。


学习速率
    学习速率是一个您设置的浮点数，用于影响模型收敛的速度。如果学习率过低，模型可能需要很长时间才能收敛。不过，如果学习速率过高，模型将永远无法收敛，而是在可最大限度减少损失的权重和偏差附近跳动。目标是选择一个既不太高也不太低的学习速率，以便模型快速收敛。


批次大小
    批次大小是一种超参数，指的是模型在更新权重和偏差之前处理的示例数量。您可能会认为，模型应先计算数据集中每个样本的损失，然后再更新权重和偏差。不过，如果数据集包含数十万甚至数百万个示例，则使用完整批次并不实际。
    
    以下两种常见技术可在不查看数据集中的每个示例的情况下，获得正确的平均梯度，然后更新权重和偏差：随机梯度下降和小批量随机梯度下降。
    随机梯度下降法 (SGD)：随机梯度下降法在每次迭代中仅使用一个示例（批次大小为 1）。在迭代次数足够多的情况下，SGD 可以正常运行，但噪声非常大。“噪声”是指训练期间导致损失在迭代过程中增加而非减少的变化。“随机”一词表示每个批次中的一个示例是随机选择的。
    
    小批次随机梯度下降法（mini-batch SGD）：小批次随机梯度下降法是全批次和 SGD 之间的折衷方案。对于 
     个数据点，批次大小可以是大于 1 且小于 
     的任意数字。模型会随机选择每个批次中包含的示例，对它们的梯度求平均值，然后在每次迭代中更新一次权重和偏差。

周期数
在训练期间，一个周期是指模型已处理训练集中的每个示例一次。例如，如果训练集包含 1,000 个示例，而小批次大小为 100 个示例，则模型需要 10 次迭代才能完成一个周期。

训练通常需要多个周期。也就是说，系统需要多次处理训练集中的每个示例。

周期数是一种超参数，您需要在模型开始训练之前设置该参数。在许多情况下，您需要通过实验来确定模型收敛所需的周期数。一般来说，训练周期数越多，模型效果越好，但训练时间也越长。


关键术语：
批次大小
Epoch
广义化
超参数
迭代
学习率
小批次
小批次随机梯度下降法
神经网络
参数
随机梯度下降法
